package dk.ku.di.dms.vms.coordinator.server.leader;

import com.google.gson.Gson;
import com.google.gson.JsonSyntaxException;
import dk.ku.di.dms.vms.coordinator.election.schema.LeaderRequest;
import dk.ku.di.dms.vms.coordinator.election.schema.VoteRequest;
import dk.ku.di.dms.vms.coordinator.metadata.ServerIdentifier;
import dk.ku.di.dms.vms.coordinator.metadata.VmsIdentifier;
import dk.ku.di.dms.vms.coordinator.server.schema.infra.ConnectionMetadata;
import dk.ku.di.dms.vms.coordinator.server.schema.internal.CommitRequest;
import dk.ku.di.dms.vms.coordinator.server.schema.internal.Heartbeat;
import dk.ku.di.dms.vms.coordinator.server.schema.internal.Presentation;
import dk.ku.di.dms.vms.coordinator.server.schema.internal.TransactionEvent;
import dk.ku.di.dms.vms.coordinator.server.schema.external.TransactionInput;
import dk.ku.di.dms.vms.coordinator.server.schema.infra.Issue;
import dk.ku.di.dms.vms.coordinator.transaction.EventIdentifier;
import dk.ku.di.dms.vms.coordinator.transaction.TransactionDAG;
import dk.ku.di.dms.vms.web_common.runnable.StoppableRunnable;

import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.*;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.locks.ReentrantLock;

import static dk.ku.di.dms.vms.coordinator.election.Constants.VOTE_REQUEST;
import static dk.ku.di.dms.vms.coordinator.server.Constants.*;
import static dk.ku.di.dms.vms.coordinator.server.schema.infra.Issue.Category.CHANNEL_NOT_REGISTERED;
import static java.net.StandardSocketOptions.SO_KEEPALIVE;
import static java.net.StandardSocketOptions.TCP_NODELAY;

public final class Leader extends StoppableRunnable {

//    private final AtomicInteger state;
//    private static final int NEW                = 0;
//    private static final int COMMITTING         = 1; //
//    private static final int ABORTING           = 2; //
//    private static final int STOPPED            = 3; //

    private static final int DEFAULT_BUFFER_SIZE = 1024;

    // a slack must be considered due to network overhead
    // e.g., by the time the timeout is reached, the time
    // taken to build the payload + sending the request over
    // the network may force the followers to initiate an
    // election. the slack is a conservative way to avoid
    // this from occurring, initiating a heartbeat sending
    // before the timeout is reached
    private int heartbeatSlack = 1000;

    // the batch window
    private long batchWindow = 60000; // a minute

    // timeout to keep track when to send heartbeats to followers
    private long heartbeatTimeout = 20000;

    // this server socket
    private final AsynchronousServerSocketChannel serverSocket;

    // group for channels
    private final AsynchronousChannelGroup group;

    // general tasks, like sending info to VMSs and other servers
    private final ExecutorService taskExecutor;

    // even though we can start with a known number of servers, their payload may have changed after a crash
    private final Map<Integer, ServerIdentifier> servers;

    private final Map<Integer, VmsIdentifier> VMSs;

    // initial design, maybe readwrite lock might be better in case several reading threads
    private final Object vmsLock;

    private final Map<Integer, ConnectionMetadata> connectionMetadataMap;

    // list of connected nodes that require identification
    // the identification comes after the first message received
    // private Map<Integer, AsynchronousSocketChannel> unknownNodeMap;

    // the identification of this server
    private final ServerIdentifier me;

    // must update the "me" on snapshotting (i.e., committing)
    private final AtomicLong tid;

    private final AtomicLong batch;

    // can be == me
    // private AtomicReference<ServerIdentifier> leader;

    // to encapsulate operations in the memory-mapped file
    // private MetadataAPI metadataAPI;

    // private Metadata metadata;

    // internal events generated by other threads, such as writers, e.g., channel closed, unreachable node, etc
    private final Queue<Issue> issueQueue;

    // transaction requests coming from the http event loop
    private final BlockingQueue<byte[]> transactionRequestsToParse;

    private final Gson gson;

    private final BlockingQueue<ByteBuffer> byteBufferQueue;

    // then another thread must take these entries and build the events
    // private Queue<TransactionInput> parsedTransactionRequests;

    // https://stackoverflow.com/questions/409932/java-timer-vs-executorservice
    // private ScheduledExecutorService scheduledBatchExecutor = Executors.newSingleThreadScheduledExecutor();

    // the heartbeat sending from the coordinator
    // private ScheduledExecutorService scheduledLeaderElectionExecutor = Executors.newSingleThreadScheduledExecutor();

    private final Map<String, TransactionDAG> transactionMap;

    public Leader(AsynchronousServerSocketChannel serverSocket,
                  AsynchronousChannelGroup group,
                  ExecutorService taskExecutor, Map<Integer,
                  ServerIdentifier> servers,
                  Map<Integer, VmsIdentifier> VMSs,
                  Map<Integer, ConnectionMetadata> connectionMetadataMap,
                  ServerIdentifier me,
                  AtomicLong tid,
                  AtomicLong batch,
                  BlockingQueue<byte[]> transactionRequestsToParse,
                  Gson gson,
                  Map<String, TransactionDAG> transactionMap) {
        super();
        this.serverSocket = Objects.requireNonNull(serverSocket);
        this.group = group;
        this.taskExecutor = Objects.requireNonNull(taskExecutor);
        this.servers = servers; // should come filled from election process
        this.VMSs = VMSs == null ? new HashMap<>() : VMSs;
        this.vmsLock = new Object();
        this.connectionMetadataMap = connectionMetadataMap == null ? new HashMap<>() : connectionMetadataMap;
        this.me = me;
        this.tid = tid == null ? new AtomicLong(0) : tid;
        this.batch = batch == null ? new AtomicLong(0) : batch;
        this.issueQueue = new LinkedBlockingQueue<>();
        this.transactionRequestsToParse = transactionRequestsToParse;
        this.gson = gson == null ? new Gson() : gson;
        this.byteBufferQueue = new LinkedBlockingQueue<>();
        this.transactionMap = Objects.requireNonNull(transactionMap); // in production, it requires receiving new transaction definitions
    }

    /**
     * Reuses the thread from the socket thread pool, instead of assigning a specific thread
     * Removes thread context switching costs.
     * This thread should not block.
     * The idea is to decode the message and deliver back to main loop as soon as possible
     */
    private class ReadCompletionHandler implements CompletionHandler<Integer, ConnectionMetadata> {

        // is it an (input) event, a heartbeat, an abort, a commit response?

        @Override
        public void completed(Integer result, ConnectionMetadata attachment) {

            // decode message


            attachment.readBuffer.clear();

            // need to read here again... how to get my channel? maybe the attachment should be the inet address so can retrieve safely from map?

            // attachment must include the channel and bytebuffer

            attachment.channel.read( attachment.readBuffer, attachment, this );
        }

        @Override
        public void failed(Throwable exc, ConnectionMetadata attachment) {
            attachment.readBuffer.clear();
        }

    }

    /**
     * This is where I define whether the connection must be kept alive
     * Depending on the nature of the request
     * https://www.baeldung.com/java-nio2-async-socket-channel
     * The first read must be a presentation message, informing what is this server (follower or VMS)
     */
    private class AcceptCompletionHandler implements CompletionHandler<AsynchronousSocketChannel, Void> {

        @Override
        public void completed(AsynchronousSocketChannel channel, Void void_) {

            ByteBuffer buffer = null;

            try {

                // do I need this check?
//                if ((channel != null) && (channel.isOpen())) {
//                }

                channel.setOption(TCP_NODELAY, true); // true disable the nagle's algorithm. not useful to have coalescence of messages in election'
                channel.setOption(SO_KEEPALIVE, true); // better to keep alive now, independently if that is a VMS or follower

                // right now I cannot discern whether it is a VMS or follower. perhaps I can keep alive channels from leader election?

                buffer = getByteBuffer();

                // read presentation message. if vms, receive metadata, if follower, nothing necessary
                Future<Integer> readFuture = channel.read( buffer );
                readFuture.get();

                if( !acceptConnection(channel, buffer) ) {
                    byteBufferQueue.add(buffer);
                }

            } catch(Exception e){
                // return buffer to queue
                if(channel != null && !channel.isOpen() && buffer != null){
                    byteBufferQueue.add(buffer);
                }
            } finally {
                // continue listening
                if (serverSocket.isOpen()){
                    serverSocket.accept(null, this);
                }
            }

        }

        @Override
        public void failed(Throwable exc, Void attachment) {
            if (serverSocket.isOpen()){
                serverSocket.accept(null, this);
            }
        }

    }

    /**
     *
     * @return A usable byte buffer
     */
    private ByteBuffer getByteBuffer() {

        ByteBuffer buffer = byteBufferQueue.poll();;

        // do we have enough bytebuffers?
         if(buffer == null){ // check if a concurrent thread has taken the available buffer
            buffer = ByteBuffer.allocateDirect(DEFAULT_BUFFER_SIZE);
        }

        return buffer;
    }

    /**
     * Task for informing the server running for leader that a leader is already established
     * We would no longer need to establish connection in case the {@link dk.ku.di.dms.vms.coordinator.election.ElectionWorker}
     * maintains the connections.
     */
    private class InformLeadershipTask implements Runnable {

        private final ServerIdentifier connectTo;

        public InformLeadershipTask(ServerIdentifier connectTo){
            this.connectTo = connectTo;
        }

        @Override
        public void run() {

            try {

                InetSocketAddress address = new InetSocketAddress(connectTo.host, connectTo.port);
                AsynchronousSocketChannel channel = AsynchronousSocketChannel.open(group);
                channel.setOption(TCP_NODELAY, true);
                channel.setOption(SO_KEEPALIVE, false);

                channel.connect(address).get();

                ByteBuffer buffer = ByteBuffer.allocate(128);

                LeaderRequest.write(buffer, me);

                channel.write(ByteBuffer.wrap(buffer.array())).get();

                if(channel.isOpen()) {
                    channel.close();
                }

            } catch(Exception ignored){}

        }

    }

    private boolean acceptConnection(AsynchronousSocketChannel channel, ByteBuffer buffer){

        // message identifier
        byte messageIdentifier = buffer.get(0);

        if(messageIdentifier == VOTE_REQUEST){
            // so I am leader, and I respond with a leader request to this new node
            // taskExecutor.submit( new ElectionWorker.WriteTask( LEADER_REQUEST, server ) );
            // would be better to maintain the connection open.....
            buffer.clear();
            ServerIdentifier serverRequestingVote = VoteRequest.read(buffer);
            taskExecutor.submit (  new InformLeadershipTask(serverRequestingVote) );
            return false;
        }

        // if it is not a presentation, drop connection
        if(messageIdentifier != PRESENTATION){
            return false;
        }

        // now let's do the work

        buffer.position(1);

        byte type = buffer.get();
        if(type == 0){
            // server
            // ....
            ServerIdentifier newServer = Presentation.readServer(buffer);

            // check whether this server is known... maybe it has crashed... then we only need to update the respective channel
            if(servers.get(newServer.hashCode()) != null){

                ConnectionMetadata connectionMetadata = connectionMetadataMap.get( newServer.hashCode() );

                // lock to refrain other threads from using old metadata
                connectionMetadata.writeLock.lock();

                // update metadata of this node
                servers.put( newServer.hashCode(), newServer );

                connectionMetadata.channel = channel;

                connectionMetadata.writeLock.unlock();

            } else { // no need for locking here

                servers.put( newServer.hashCode(), newServer );

                ConnectionMetadata connectionMetadata = new ConnectionMetadata( buffer, getByteBuffer(), channel, new ReentrantLock() );
                connectionMetadataMap.put( newServer.hashCode(), connectionMetadata );
                // create a read handler for this connection
                // attach buffer, so it can be read upon completion
                channel.read(buffer, connectionMetadata, new ReadCompletionHandler());

            }
        } else if(type == 1){ // vms

            VmsIdentifier newVms = Presentation.readVms(buffer, gson);

            if(VMSs.get( newVms.hashCode() ) != null){
                // vms reconnecting

                ConnectionMetadata connectionMetadata = connectionMetadataMap.get( newVms.hashCode() );

                // lock to refrain other threads from using old metadata
                connectionMetadata.writeLock.lock();

                // update metadata of this node
                VMSs.put( newVms.hashCode(), newVms );

                connectionMetadata.channel = channel;

                connectionMetadata.writeLock.unlock();

            } else {

                VMSs.put( newVms.hashCode(), newVms );

                ConnectionMetadata connectionMetadata = new ConnectionMetadata( buffer, getByteBuffer(), channel, new ReentrantLock() );
                connectionMetadataMap.put( newVms.hashCode(), connectionMetadata );

                channel.read(buffer, connectionMetadata, new ReadCompletionHandler());

            }

        } else {
            // simply unknown... probably a bug
            try{
                if(channel.isOpen()) {
                    channel.close();
                }
            } catch(Exception ignored){}
            return false;

        }

        return true;
    }

    /**
     * Given a set of VMSs involved in the last batch
     * (for easiness can send to all of them for now)
     * send a batch request.
     */
    private final class CommitHandler implements Runnable {

        // constructor: all metadata

        @Override
        public void run() {



            ByteBuffer buffer = getByteBuffer();

            // we need a cut. all vms must be aligned in terms of tid
            // because the other thread might still be sending intersecting TIDs
            // e.g., vms 1 receives batch 1 tid 1 vms 2 received batch 2 tid 1
            // this is solved by fixing the batch per transaction (and all its events)
            // this is an anomaly across batches

            // but another problem is that an interleaving can allow the following problem
            // commit handler issues batch 1 to vms 1 with last tid 1
            // but tx-mgr issues event with batch 1 vms 1 last tid 2
            // this can happen because the tx-mgr is still in the loop
            // this is an anomaly within a batch

            // so we need synchronization to disallow the second

            long currBatch = batch.get();

            synchronized (vmsLock) {

                long newBatch = batch.incrementAndGet();
                logger.info("Current batch is "+currBatch+" and new batch is "+newBatch);

                for (VmsIdentifier vms : VMSs.values()) {



                    CommitRequest.write(buffer, vms.lastTid, currBatch);

                }

            }

            // if write fails, it is because

            // close offset

            // get all vms that participated in the last batch

            // send commit info

            // wait for all ACKs given a timestamp


        }

    }

    /**
     * Different logic compared to {@link WriteCompletionHandler}
     * In case of any ABORT received for the batch, must write an ABORT
     * event to all involved VMSs
     */
    private static class CommitCompletionHandler implements CompletionHandler<Integer, ConnectionMetadata> {

        // data structure to keep track of which VMSs have committed

        @Override
        public void completed(Integer result, ConnectionMetadata connectionMetadata) {
            connectionMetadata.writeBuffer.clear();
            connectionMetadata.writeLock.unlock();
        }

        @Override
        public void failed(Throwable exc, ConnectionMetadata connectionMetadata) {
            connectionMetadata.writeBuffer.clear();
            connectionMetadata.writeLock.unlock();
        }

    }

    /**
     * This method contains the main loop that contains the main functions of a leader
     * (a) Heartbeat sending to avoid followers to initiate a leader election. That can still happen due to network latency.
     *  What happens if two nodes declare themselves as leaders? We need some way to let it know
     * (b) Batch management
     * ------ NO ----- (c) Processing of transaction requests
     * designing leader mode first
     * design follower mode in another class to avoid convoluted code
     */
    @Override
    public void run() {

        // ServerSocketChannel.open().register()

        // the batch handler must synchronize with the heartbeat??? heartbeat is only for followers? heartbeat also from VMSs

        long lastBatchTimestamp = System.currentTimeMillis();
        long lastHeartbeat = lastBatchTimestamp;

        long elapsed;

        // do I need this variable? not if it is leader
        // int state_ = state.get();

        // send heartbeat to all VMSs too.... ?
        sendHeartbeats();

        // going for a different socket to allow for heterogeneous ways for a client to connect with the servers e.g., http.
        // it is also good to better separate resources, so VMSs and followers do not share resources with external clients

        // setup asynchronous listener for new connections
        serverSocket.accept( null, new AcceptCompletionHandler());

        taskExecutor.submit( new TransactionManager() );

        while(!isStopped()){

            long now = System.currentTimeMillis();

            elapsed = now - lastHeartbeat;

            if ( elapsed >= heartbeatTimeout - heartbeatSlack ){
                lastHeartbeat = now + heartbeatSlack;
                // send heartbeat to all followers
                sendHeartbeats();
            }

            // is batch time?
            if( now - lastBatchTimestamp >= batchWindow ){
                lastBatchTimestamp = now;
                // should keep track which events must be included in the batch, remember there must be concurrent threads reading new transactions
                Future<?> batchTaskFuture = taskExecutor.submit( new CommitHandler() );

                try {
                    batchTaskFuture.get();
                } catch (InterruptedException | ExecutionException ignored) {}

            }

            logger.info("I am "+me.host+":"+me.port);
        }

        // must find a way to stop the accepting

    }

    /**
     * Given a list of known followers, send to each a heartbeat
     * Heartbeats must have precedence over other writes, since they
     * avoid the overhead of starting a new election process in remote nodes
     * and generating new messages over the network.
     *
     * I can implement later a priority-based scheduling of writes.... maybe some Java DT can help?
     */
    private void sendHeartbeats() {
        logger.info("Sending vote requests. I am "+ me.host+":"+me.port);
        for(ServerIdentifier server : servers.values()){
            ConnectionMetadata connectionMetadata = connectionMetadataMap.get( server.hashCode() );
            AsynchronousSocketChannel channel = connectionMetadata.channel;
            if(channel != null) {
                Heartbeat.write(connectionMetadata.writeBuffer, me);
                connectionMetadata.channel.write( connectionMetadata.writeBuffer, connectionMetadata, new WriteCompletionHandler() );
            } else {
                issueQueue.add( new Issue( CHANNEL_NOT_REGISTERED, server ) );
            }
        }
    }

    /**
     * Allows to reuse the thread pool assigned to socket to complete the writing
     * That refrains the main thread and the TransactionManager to block, thus allowing its progress
     */
    private static final class WriteCompletionHandler implements CompletionHandler<Integer, ConnectionMetadata> {

        @Override
        public void completed(Integer result, ConnectionMetadata connectionMetadata) {
            connectionMetadata.writeBuffer.clear();
            connectionMetadata.writeLock.unlock();
        }

        @Override
        public void failed(Throwable exc, ConnectionMetadata connectionMetadata) {
            connectionMetadata.writeBuffer.clear();
            connectionMetadata.writeLock.unlock();
        }

    }

    /**
     * This task assumes the channels are already established
     * Cannot have two threads writing to the same channel at the same time
     * A transaction manager is responsible for assigning TIDs to incoming transaction requests
     * This task also involves making sure the writes are performed successfully
     * A writer manager is responsible for defining strategies, policies, safety guarantees on
     * writing concurrently to channels.
     */
    private class TransactionManager extends StoppableRunnable {

        @Override
        public void run() {

            Collection<byte[]> drainedElements = new ArrayList<>(100000); // 100000 requests per second can be handled

            while(!isStopped()){

                // should read in a proportion that matches the batch and heartbeat window, otherwise
                // how long does it take to process a batch of input transactions?
                // instead of trying to match the rate of processing, perhaps we can create read tasks
                if (transactionRequestsToParse.size() > 0) {
                    // the payload received in bytes is json format

                    transactionRequestsToParse.drainTo(drainedElements);

                    List<TransactionInput> parsedTransactionRequests = new ArrayList<>( drainedElements.size() );

                    // do not send another until the last has been completed, so we can better adjust the rate of parsing
                    for (byte[] drainedElement : drainedElements) {
                        String json = new String(drainedElement);

                        // must also check whether the event is correct, that is, contains all events
                        try {
                            TransactionInput transactionInput = gson.fromJson(json, TransactionInput.class);
                            parsedTransactionRequests.add(transactionInput);
                        } catch(JsonSyntaxException ignored) {} // simply ignore
                    }

                    drainedElements.clear();

                    for(TransactionInput transactionInput : parsedTransactionRequests){

                        long tid_ = tid.addAndGet(1);

                        TransactionDAG transaction = transactionMap.get( transactionInput.name );

                        // TIDI -> get a snapshot of tids and batch for all transactionInput.events

                        // to allow for a snapshot of the last TIDs of each vms involved in this transaction
                        synchronized (vmsLock) {

                            // should find a way to continue emitting new transactions without stopping this thread
                            // non-blocking design
                            // having the batch here guarantees that all input events of the same tid does
                            // not belong to different batches
                            long batch_ = batch.get();

                            // for each input event, send the event to the proper vms
                            // assuming the input is correct, i.e., all events are present
                            for (TransactionInput.Event inputEvent : transactionInput.events) {

                                // look for the event in the topology
                                EventIdentifier event = transaction.topology.get(inputEvent.event);

                                // get the vms
                                VmsIdentifier vms = VMSs.get(event.vms.hashCode());

                                // get the connection metadata
                                ConnectionMetadata connectionMetadata = connectionMetadataMap.get(Objects.hash(vms.host, vms.port));

                                // we could employ deterministic writes to the channel, that is, an order that would not require locking for writes
                                connectionMetadata.writeLock.lock();

                                // assign this event, so... what? try to send later?
                                connectionMetadata.lastEventWritten = event;

                                // write. think about failures/atomicity later
                                TransactionEvent.write(connectionMetadata.writeBuffer, tid_, vms.lastTid, batch_, inputEvent.event, inputEvent.payload);

                                // a vms, although receiving an event from a "next" batch, cannot yet commit, since
                                // there may have additional events to arrive from the current batch
                                // so the batch request must contain the last tid of the given vms

                                // update for next transaction
                                vms.lastTid = tid_;

                                connectionMetadata.channel.write(connectionMetadata.writeBuffer, connectionMetadata, new WriteCompletionHandler());

                            }
                        }

                    }

                }

            }

        }

    }

}
