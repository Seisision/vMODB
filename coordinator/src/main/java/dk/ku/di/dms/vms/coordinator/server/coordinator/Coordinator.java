package dk.ku.di.dms.vms.coordinator.server.coordinator;

import com.google.gson.Gson;
import com.google.gson.JsonSyntaxException;
import dk.ku.di.dms.vms.coordinator.election.schema.LeaderRequest;
import dk.ku.di.dms.vms.coordinator.election.schema.VoteRequest;
import dk.ku.di.dms.vms.coordinator.metadata.ServerIdentifier;
import dk.ku.di.dms.vms.coordinator.metadata.VmsIdentifier;
import dk.ku.di.dms.vms.coordinator.server.schema.batch.BatchComplete;
import dk.ku.di.dms.vms.coordinator.server.schema.transaction.TransactionAbort;
import dk.ku.di.dms.vms.coordinator.server.schema.batch.BatchReplication;
import dk.ku.di.dms.vms.coordinator.server.infra.ConnectionMetadata;
import dk.ku.di.dms.vms.coordinator.server.infra.VmsConnectionMetadata;
import dk.ku.di.dms.vms.coordinator.server.schema.internal.*;
import dk.ku.di.dms.vms.coordinator.server.schema.external.TransactionInput;
import dk.ku.di.dms.vms.coordinator.server.infra.Issue;
import dk.ku.di.dms.vms.coordinator.server.schema.transaction.TransactionEvent;
import dk.ku.di.dms.vms.coordinator.transaction.EventIdentifier;
import dk.ku.di.dms.vms.coordinator.transaction.TransactionDAG;
import dk.ku.di.dms.vms.web_common.runnable.SignalingStoppableRunnable;
import dk.ku.di.dms.vms.web_common.runnable.StoppableRunnable;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.*;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.locks.ReentrantLock;
import java.util.stream.Collectors;

import static dk.ku.di.dms.vms.coordinator.election.Constants.VOTE_REQUEST;
import static dk.ku.di.dms.vms.coordinator.server.coordinator.BatchReplicationStrategy.*;
import static dk.ku.di.dms.vms.coordinator.server.infra.Constants.*;
import static dk.ku.di.dms.vms.coordinator.server.infra.ConnectionMetadata.NodeType.SERVER;
import static dk.ku.di.dms.vms.coordinator.server.infra.ConnectionMetadata.NodeType.VMS;
import static dk.ku.di.dms.vms.coordinator.server.infra.Issue.Category.CHANNEL_NOT_REGISTERED;
import static dk.ku.di.dms.vms.coordinator.server.infra.Issue.Category.UNREACHABLE_NODE;
import static java.net.StandardSocketOptions.SO_KEEPALIVE;
import static java.net.StandardSocketOptions.TCP_NODELAY;

/**
 * Class that encapsulates all logic related to issuing of
 * batch commits, transaction aborts, ...
 */
public final class Coordinator extends SignalingStoppableRunnable {

    private static final int DEFAULT_BUFFER_SIZE = 1024;

    private final CoordinatorOptions options;

    // this server socket
    private final AsynchronousServerSocketChannel serverSocket;

    // group for channels
    private final AsynchronousChannelGroup group;

    // general tasks, like sending info to VMSs and other servers
    private final ExecutorService taskExecutor;

    // even though we can start with a known number of servers, their payload may have changed after a crash
    private final Map<Integer, ServerIdentifier> servers;

    // for server nodes
    private final Map<Integer, ConnectionMetadata> serverConnectionMetadataMap;

    /** VMS data structures **/
    private final Map<Integer, VmsIdentifier> VMSs;

    private final Map<Integer, VmsConnectionMetadata> vmsConnectionMetadataMap;

    // the identification of this server
    private final ServerIdentifier me;

    // must update the "me" on snapshotting (i.e., committing)
    private volatile long tid;

    // initial design, maybe readwrite lock might be better in case several reading threads
    private final Object batchCommitLock;

    // the offset of the pending batch commit
    private volatile long batchOffsetPendingCommit;

    // the current batch on which new transactions are being generated for
    private volatile long batchOffset;

    // metadata about all non-committed batches. when a batch commit finishes, it is removed from this map
    private final Map<Long, BatchContext> batchContextMap;

    private final BatchReplicationStrategy batchReplicationStrategy;

    // can be == me
    // private AtomicReference<ServerIdentifier> leader;

    // to encapsulate operations in the memory-mapped file
    // private MetadataAPI metadataAPI;

    // private Metadata metadata;

    // internal events generated by other threads, such as writers, e.g., channel closed, unreachable node, etc
    private final BlockingQueue<Issue> issueQueue;

    // transaction requests coming from the http event loop
    private final BlockingQueue<byte[]> transactionRequestsToParse;

    // to reuse the same collection to avoid creating collection every run
    // maybe this can be off-heap???
    private final Collection<byte[]> drainedTransactionRequests; // 100000 requests per second can be handled

    private final TransactionManagerContext txManagerCtx;

    private record TransactionManagerContext (
            // transaction manager actions
            // this provides a natural separation of tasks in the transaction manager thread. commit handling, transaction parsing, leaving the main thread free (only sending heartbeats)
            BlockingQueue<Byte> transactionManagerActionQueue,
            Queue<BatchComplete.BatchCompletePayload> batchCompleteEvents,
            Queue<TransactionAbort.TransactionAbortPayload> transactionAbortEvents
    ){}

    private final Gson gson;

    private final BlockingQueue<ByteBuffer> byteBufferQueue;

    // then another thread must take these entries and build the events
    // private Queue<TransactionInput> parsedTransactionRequests;

    // https://stackoverflow.com/questions/409932/java-timer-vs-executorservice
    // private ScheduledExecutorService scheduledBatchExecutor = Executors.newSingleThreadScheduledExecutor();

    // the heartbeat sending from the coordinator
    // private ScheduledExecutorService scheduledLeaderElectionExecutor = Executors.newSingleThreadScheduledExecutor();

    private final Map<String, TransactionDAG> transactionMap;

    public Coordinator(AsynchronousServerSocketChannel serverSocket,
                       AsynchronousChannelGroup group,
                       ExecutorService taskExecutor,
                       Map<Integer, ServerIdentifier> servers,
                       Map<Integer, VmsIdentifier> VMSs,
                       Map<Integer, ConnectionMetadata> serverConnectionMetadataMap,
                       ServerIdentifier me,
                       CoordinatorOptions options,
                       long startingTid,
                       long batchOffset,
                       BatchReplicationStrategy batchReplicationStrategy,
                       BlockingQueue<byte[]> transactionRequestsToParse,
                       Gson gson,
                       Map<String, TransactionDAG> transactionMap) {
        super();

        // network and executor
        this.serverSocket = Objects.requireNonNull(serverSocket);
        this.group = group;
        this.taskExecutor = Objects.requireNonNull(taskExecutor);

        // should come filled from election process
        this.servers = servers == null ? new ConcurrentHashMap<>() : servers;
        this.VMSs = VMSs == null ? new ConcurrentHashMap<>() : VMSs;
        this.batchCommitLock = new Object();

        // might come filled from election process
        this.serverConnectionMetadataMap = serverConnectionMetadataMap == null ? new HashMap<>() : serverConnectionMetadataMap;
        this.vmsConnectionMetadataMap = new HashMap<>();
        this.me = me;

        // infra
        this.issueQueue = new LinkedBlockingQueue<>();
        this.gson = gson == null ? new Gson() : gson;
        this.byteBufferQueue = new LinkedBlockingQueue<>();

        // coordinator options
        this.options = options;

        // transactions
        this.tid = startingTid;
        this.transactionRequestsToParse = transactionRequestsToParse; // shared data structure
        this.drainedTransactionRequests = new ArrayList<>(100000); // collec
        this.transactionMap = Objects.requireNonNull(transactionMap); // in production, it requires receiving new transaction definitions
        this.txManagerCtx = new TransactionManagerContext(
                new LinkedBlockingQueue<>(),
                new ConcurrentLinkedQueue<>(),
                new ConcurrentLinkedQueue<>() );

        // batch
        this.batchOffset = batchOffset;
        this.batchOffsetPendingCommit = batchOffset;
        this.batchContextMap = new ConcurrentHashMap<>();
        this.batchReplicationStrategy = batchReplicationStrategy;
    }

    /**
     * Reuses the thread from the socket thread pool, instead of assigning a specific thread
     * Removes thread context switching costs.
     * This thread should not block.
     * The idea is to decode the message and deliver back to main loop as soon as possible
     *
     * This thread must be set free as soon as possible
     */
    private class ReadCompletionHandler implements CompletionHandler<Integer, ConnectionMetadata> {

        // is it an abort, a commit response?
        // it cannot be replication because i have opened another channel for that

        @Override
        public void completed(Integer result, ConnectionMetadata connectionMetadata) {

            // decode message by getting the first byte
            byte type = connectionMetadata.readBuffer.get();

            // from all terminal VMSs involved in the last batch
            if(type == BATCH_COMPLETE){

                // don't actually need the host and port in the payload since we have the attachment to this read operation...
                BatchComplete.BatchCompletePayload response = BatchComplete.read( connectionMetadata.readBuffer );
                txManagerCtx.batchCompleteEvents().add( response );
                txManagerCtx.transactionManagerActionQueue.add(BATCH_COMPLETE); // must have a context, i.e., what batch, the last?

                // if one abort, no need to keep receiving
                // actually it is unclear in which circumstances a vms would respond no... probably in case it has not received an ack from an aborted commit response?
                // because only the aborted transaction will be rolled back

            } else if (type == TX_ABORT){

                // get information of what
                TransactionAbort.TransactionAbortPayload response = TransactionAbort.read(connectionMetadata.readBuffer);
                txManagerCtx.transactionAbortEvents().add( response );
                txManagerCtx.transactionManagerActionQueue.add(TX_ABORT);

            } else {
                logger.warning("Unknown message received.");
            }

            connectionMetadata.readBuffer.clear();

            connectionMetadata.channel.read( connectionMetadata.readBuffer, connectionMetadata, this );
        }

        @Override
        public void failed(Throwable exc, ConnectionMetadata connectionMetadata) {
            connectionMetadata.readBuffer.clear();
            if(connectionMetadata.channel.isOpen()){
                connectionMetadata.channel.read( connectionMetadata.readBuffer, connectionMetadata, this );
            } else {

                // modify status
                if(connectionMetadata.nodeType == VMS){
                    VMSs.get(connectionMetadata.key).active = false;
                } else {
                    servers.get(connectionMetadata.key).active = false;
                }

            }
        }

    }

    /**
     * This is where I define whether the connection must be kept alive
     * Depending on the nature of the request
     * https://www.baeldung.com/java-nio2-async-socket-channel
     * The first read must be a presentation message, informing what is this server (follower or VMS)
     */
    private class AcceptCompletionHandler implements CompletionHandler<AsynchronousSocketChannel, Void> {

        @Override
        public void completed(AsynchronousSocketChannel channel, Void void_) {

            ByteBuffer buffer = null;

            try {

                // do I need this check? I believe that if the operation completed and keep alive connection, this is always true
//                if ((channel != null) && (channel.isOpen())) {}

                channel.setOption(TCP_NODELAY, true); // true disable the nagle's algorithm. not useful to have coalescence of messages in election'
                channel.setOption(SO_KEEPALIVE, true); // better to keep alive now, independently if that is a VMS or follower

                // right now I cannot discern whether it is a VMS or follower. perhaps I can keep alive channels from leader election?

                buffer = getByteBuffer();

                // read presentation message. if vms, receive metadata, if follower, nothing necessary
                Future<Integer> readFuture = channel.read( buffer );
                readFuture.get();

                if( !acceptConnection(channel, buffer) ) {
                    byteBufferQueue.add(buffer);
                }

            } catch(Exception e){
                // return buffer to queue
                if(channel != null && !channel.isOpen() && buffer != null){
                    byteBufferQueue.add(buffer);
                }
            } finally {
                // continue listening
                if (serverSocket.isOpen()){
                    serverSocket.accept(null, this);
                }
            }

        }

        @Override
        public void failed(Throwable exc, Void attachment) {
            if (serverSocket.isOpen()){
                serverSocket.accept(null, this);
            }
        }

    }

    /**
     *
     * @return A usable byte buffer
     */
    private ByteBuffer getByteBuffer() {

        ByteBuffer buffer = byteBufferQueue.poll();

        // do we have enough bytebuffers?
         if(buffer == null){ // check if a concurrent thread has taken the available buffer
            buffer = ByteBuffer.allocateDirect(DEFAULT_BUFFER_SIZE);
        }

        return buffer;
    }

    private void returnByteBuffer(ByteBuffer buffer){
        byteBufferQueue.add(buffer);
    }

    /**
     * Task for informing the server running for leader that a leader is already established
     * We would no longer need to establish connection in case the {@link dk.ku.di.dms.vms.coordinator.election.ElectionWorker}
     * maintains the connections.
     */
    private class InformLeadershipTask implements Runnable {

        private final ServerIdentifier connectTo;

        public InformLeadershipTask(ServerIdentifier connectTo){
            this.connectTo = connectTo;
        }

        @Override
        public void run() {

            try {

                InetSocketAddress address = new InetSocketAddress(connectTo.host, connectTo.port);
                AsynchronousSocketChannel channel = AsynchronousSocketChannel.open(group);
                channel.setOption(TCP_NODELAY, true);
                channel.setOption(SO_KEEPALIVE, false);

                channel.connect(address).get();

                ByteBuffer buffer = ByteBuffer.allocate(128);

                LeaderRequest.write(buffer, me);

                channel.write(ByteBuffer.wrap(buffer.array())).get();

                if(channel.isOpen()) {
                    channel.close();
                }

            } catch(Exception ignored){}

        }

    }

    private boolean acceptConnection(AsynchronousSocketChannel channel, ByteBuffer buffer){

        // message identifier
        byte messageIdentifier = buffer.get(0);

        if(messageIdentifier == VOTE_REQUEST){
            // so I am leader, and I respond with a leader request to this new node
            // taskExecutor.submit( new ElectionWorker.WriteTask( LEADER_REQUEST, server ) );
            // would be better to maintain the connection open.....
            buffer.clear();
            ServerIdentifier serverRequestingVote = VoteRequest.read(buffer);
            taskExecutor.submit (  new InformLeadershipTask(serverRequestingVote) );
            return false;
        }

        // if it is not a presentation, drop connection
        if(messageIdentifier != PRESENTATION){
            return false;
        }

        // now let's do the work

        buffer.position(1);

        byte type = buffer.get();
        if(type == 0){
            // server
            // ....
            ServerIdentifier newServer = Presentation.readServer(buffer);

            // check whether this server is known... maybe it has crashed... then we only need to update the respective channel
            if(servers.get(newServer.hashCode()) != null){

                ConnectionMetadata connectionMetadata = serverConnectionMetadataMap.get( newServer.hashCode() );

                // lock to refrain other threads from using old metadata
                connectionMetadata.writeLock.lock();

                // update metadata of this node
                servers.put( newServer.hashCode(), newServer );

                connectionMetadata.channel = channel;

                connectionMetadata.writeLock.unlock();

            } else { // no need for locking here

                servers.put( newServer.hashCode(), newServer );

                ConnectionMetadata connectionMetadata = new ConnectionMetadata( newServer.hashCode(), SERVER, buffer, getByteBuffer(), channel, new ReentrantLock() );
                serverConnectionMetadataMap.put( newServer.hashCode(), connectionMetadata );
                // create a read handler for this connection
                // attach buffer, so it can be read upon completion
                channel.read(buffer, connectionMetadata, new ReadCompletionHandler());

            }
        } else if(type == 1){ // vms

            VmsIdentifier newVms = Presentation.readVms(buffer, gson);

            if(VMSs.get( newVms.hashCode() ) != null){
                // vms reconnecting

                VmsConnectionMetadata connectionMetadata = vmsConnectionMetadataMap.get( newVms.hashCode() );

                // lock to refrain other threads from using old metadata
                connectionMetadata.writeLock.lock();

                // update metadata of this node
                VMSs.put( newVms.hashCode(), newVms );

                connectionMetadata.channel = channel;

                connectionMetadata.writeLock.unlock();

            } else {
                VMSs.put( newVms.hashCode(), newVms );
                VmsConnectionMetadata connectionMetadata = new VmsConnectionMetadata( newVms.hashCode(), buffer, getByteBuffer(), channel, new ReentrantLock() );
                vmsConnectionMetadataMap.put( newVms.hashCode(), connectionMetadata );
                channel.read(buffer, connectionMetadata, new ReadCompletionHandler());
            }

        } else {
            // simply unknown... probably a bug?
            try{
                if(channel.isOpen()) {
                    channel.close();
                }
            } catch(Exception ignored){}
            return false;

        }

        return true;
    }

    /**
     * Given a set of VMSs involved in the last batch
     * (for easiness can send to all of them for now)
     * send a batch request.
     */
    private final class BatchCommitWorker implements Runnable {

        @Override
        public void run() {

            // we need a cut. all vms must be aligned in terms of tid
            // because the other thread might still be sending intersecting TIDs
            // e.g., vms 1 receives batch 1 tid 1 vms 2 received batch 2 tid 1
            // this is solved by fixing the batch per transaction (and all its events)
            // this is an anomaly across batches

            // but another problem is that an interleaving can allow the following problem
            // commit handler issues batch 1 to vms 1 with last tid 1
            // but tx-mgr issues event with batch 1 vms 1 last tid 2
            // this can happen because the tx-mgr is still in the loop
            // this is an anomaly within a batch

            // so we need synchronization to disallow the second

            // obtain a consistent snapshot of last tids for all VMSs
            // the transaction manager must obtain the next batch inside the synchronized block

            // why do I need to replicate vmsTidMap? to restart from this point if the leader fails
            Map<String,Long> vmsTidMap;
            long currBatch;
            synchronized (batchCommitLock) {

                currBatch = batchOffset;

                BatchContext currBatchContext = batchContextMap.get( currBatch );
                currBatchContext.seal();

                // define new batch context
                // need to get
                BatchContext newBatchContext = new BatchContext(batchOffset);
                batchContextMap.put( batchOffset, newBatchContext );

                long newBatch = ++batchOffset; // first increase the value and then execute the statement

                // a map of the last tid for each vms
                vmsTidMap = VMSs.values().stream().collect(
                        Collectors.toMap( VmsIdentifier::getName, VmsIdentifier::getLastTid ) );

                logger.info("Current batch offset is "+currBatch+" and new batch offset is "+newBatch);

            }
            // new TIDs will be emitted with the new batch in the transaction manager

            // to refrain the number of servers increasing concurrently, instead of
            // synchronizing the operation, I can simply obtain the collection first
            // but what happens if one of the servers in the list fails?
            Collection<ServerIdentifier> activeServers = servers.values();
            int nServers = activeServers.size();

            CompletableFuture<?>[] promises = new CompletableFuture[nServers];

            Set<Integer> serverVotes = Collections.synchronizedSet( new HashSet<>(nServers) );

            int i = 0;
            for(ServerIdentifier server : activeServers){

                if(!server.active) continue;
                promises[i] = CompletableFuture.supplyAsync( () ->
                {
                    // could potentially use another channel for writing commit-related messages...
                    // could also just open and close a new connection
                    // actually I need this since I must read from this thread instead of relying on the
                    // read completion handler
                    AsynchronousSocketChannel channel = null;
                    try {

                        InetSocketAddress address = new InetSocketAddress(server.host, server.port);
                        channel = AsynchronousSocketChannel.open(group);
                        channel.setOption( TCP_NODELAY, true );
                        channel.setOption( SO_KEEPALIVE, false );
                        channel.connect(address).get();

                        ByteBuffer buffer = getByteBuffer();
                        BatchReplication.write( buffer, currBatch, gson.toJson( vmsTidMap ) );
                        channel.write( buffer ).get();

                        buffer.clear();

                        // immediate read in the same channel
                        channel.read( buffer ).get();

                        // CommitResponse.CommitResponsePayload response = CommitResponse.read( buffer );

                        buffer.clear();

                        returnByteBuffer(buffer);

                        // assuming the follower always accept, even though it came back from failure and has old metadata
                        serverVotes.add( server.hashCode() );

                        return null;

                    } catch (InterruptedException | ExecutionException | IOException e) {
                        // cannot connect to host
                        logger.warning("Error connecting to host. I am "+ me.host+":"+me.port+" and the target is "+ server.host+":"+ server.port);
                        return null;
                    } finally {
                        if(channel != null && channel.isOpen()) {
                            try {
                                channel.close();
                            } catch (IOException ignored) {}
                        }
                    }

                    // these threads need to block to wait for server response

                }, taskExecutor).exceptionallyAsync( (x) -> {
                    logError(server);
                    return null;
                }, taskExecutor);
                i++;
            }

            if ( batchReplicationStrategy == ONE ){
                // asynchronous
                // at least one is always necessary
                int j = 0;
                while (j < nServers && serverVotes.size() < 1){
                    promises[i].join();
                    j++;
                }
                if(serverVotes.isEmpty()){
                    logger.warning("The system has entered in a state that data may be lost since there are no followers to replicate the current batch offset.");
                }
            } else if ( batchReplicationStrategy == MAJORITY ){

                int simpleMajority = ((nServers + 1) / 2);
                // idea is to iterate through servers, "joining" them until we have enough
                int j = 0;
                while (j < nServers && serverVotes.size() <= simpleMajority){
                    promises[i].join();
                    j++;
                }

                if(serverVotes.size() < simpleMajority){
                    logger.warning("The system has entered in a state that data may be lost since a majority have not been obtained to replicate the current batch offset.");
                }
            } else if ( batchReplicationStrategy == ALL ) {
                CompletableFuture.allOf( promises ).join();
                if ( serverVotes.size() < nServers ) {
                    logger.warning("The system has entered in a state that data may be lost since there are missing votes to replicate the current batch offset.");
                }
            }

            // for now, we don't have a fallback strategy...

        }

        private void logError(ServerIdentifier server) {
            issueQueue.add( new Issue( UNREACHABLE_NODE, server ) );
        }

    }

    /**
     * Data structure to keep data about the current batch commit
     */
    private static class BatchContext {

        // no need non volatile. immutable
        public final long batchOffset;

        // set of terminal VMSs that has not voted yet
        public Set<String> missingVotes;

        // may change across batches
        public Set<String> terminalVMSs;

        public BatchContext(long batchOffset) {
            this.batchOffset = batchOffset;
            this.terminalVMSs = Collections.synchronizedSet(new HashSet<>());
        }

        // called when the batch is over
        public void seal(){
            this.missingVotes = Collections.unmodifiableSet(terminalVMSs);
        }

    }

    /**
     * This method contains the main loop that contains the main functions of a leader
     *  NOT ANYMORE (a) Heartbeat sending to avoid followers to initiate a leader election. That can still happen due to network latency.
     *  What happens if two nodes declare themselves as leaders? We need some way to let it know
     *  OK (b) Batch management
     * ------ NO ----- (c) Processing of transaction requests
     * designing leader mode first
     * design follower mode in another class to avoid convoluted code
     */
    @Override
    public void run() {

        long now = System.nanoTime();
        long lastBatchTimestamp = now;

        // going for a different socket to allow for heterogeneous ways for a client to connect with the servers e.g., http.
        // it is also good to better separate resources, so VMSs and followers do not share resources with external clients

        // setup asynchronous listener for new connections
        serverSocket.accept( null, new AcceptCompletionHandler());

        // only submit when there are events to react to
        // perhaps not a good idea to have this thread in a pool, since this thread will get blocked
        // BUT, it issimply about increasing the pool size with +1...
        TransactionManager txManager = new TransactionManager();
        taskExecutor.submit( txManager );

        ScheduledExecutorService parseExecutorService = Executors.newSingleThreadScheduledExecutor();
        ScheduledFuture<?> parseTask = parseExecutorService.schedule(this::parseAndSendTransactionInputEvents, options.getParseTimeout(), TimeUnit.MILLISECONDS);

        ScheduledExecutorService heartbeatExecutorService = Executors.newSingleThreadScheduledExecutor();
        ScheduledFuture<?> heartbeatTask = heartbeatExecutorService.schedule(this::sendHeartbeats, options.getHeartbeatTimeout(), TimeUnit.MILLISECONDS);

        // if the transaction manager thread blocks (e.g., waiting for a queue), the thread is not delivered back to the pool

        while(!isStopped()){

            // how do I track whether the last batch has finished or not?
            // there should be a maximum of waiting time
            // at first we can assume VMSs never fail, but batches cannot intersect
            long elapsed = now - lastBatchTimestamp;
            // is batch time?
            if( elapsed >= options.getBatchWindow() ){
                lastBatchTimestamp = now;

                // another design is generating an event to the transaction manager
                // that would avoid synchronization

                // should keep track which events must be included in the batch, remember there must be concurrent threads reading new transactions
                taskExecutor.submit( new BatchCommitWorker() );

            }

            // logger.info("I am "+me.host+":"+me.port);

            now = System.nanoTime();
        }

        // safe close
        heartbeatTask.cancel(false); // do not interrupt given the lock management
        parseTask.cancel(false);
        txManager.stop();
        try { serverSocket.close(); } catch (IOException ignored) {}

    }

    /**
     * Given a list of known followers, send to each a heartbeat
     * Heartbeats must have precedence over other writes, since they
     * avoid the overhead of starting a new election process in remote nodes
     * and generating new messages over the network.
     *
     * I can implement later a priority-based scheduling of writes.... maybe some Java DT can help?
     */
    private void sendHeartbeats() {
        logger.info("Sending vote requests. I am "+ me.host+":"+me.port);
        for(ServerIdentifier server : servers.values()){
            ConnectionMetadata connectionMetadata = serverConnectionMetadataMap.get( server.hashCode() );
            if(connectionMetadata.channel != null) {
                Heartbeat.write(connectionMetadata.writeBuffer, me);
                connectionMetadata.writeLock.lock();
                connectionMetadata.channel.write( connectionMetadata.writeBuffer, connectionMetadata, new WriteCompletionHandler() );
            } else {
                issueQueue.add( new Issue( CHANNEL_NOT_REGISTERED, server ) );
            }
        }
    }

    /**
     * Allows to reuse the thread pool assigned to socket to complete the writing
     * That refrains the main thread and the TransactionManager to block, thus allowing its progress
     */
    private static final class WriteCompletionHandler implements CompletionHandler<Integer, ConnectionMetadata> {

        @Override
        public void completed(Integer result, ConnectionMetadata connectionMetadata) {
            connectionMetadata.writeBuffer.clear();
            connectionMetadata.writeLock.unlock();
        }

        @Override
        public void failed(Throwable exc, ConnectionMetadata connectionMetadata) {
            connectionMetadata.writeBuffer.clear();
            connectionMetadata.writeLock.unlock();
        }

    }

    /**
     * This task assumes the channels are already established
     * Cannot have two threads writing to the same channel at the same time
     * A transaction manager is responsible for assigning TIDs to incoming transaction requests
     * This task also involves making sure the writes are performed successfully
     * A writer manager is responsible for defining strategies, policies, safety guarantees on
     * writing concurrently to channels.
     */
    private class TransactionManager extends StoppableRunnable {

//        private volatile int state;
//        public static final int WAITING_FOR_BATCH_INIT          = 0;
//        public static final int REPLICATING_BATCH_METADATA      = 1;
//        public static final int SENDING_PREPARE_TO_VMS          = 2; // running the protocol
//        public static final int CLOSING_CURRENT_BATCH           = 3; // has received the ACKs from a majority

        @Override
        public void run() {

            while(!isStopped()){

                try {
                    // https://web.mit.edu/6.005/www/fa14/classes/20-queues-locks/message-passing/
                    byte action = txManagerCtx.transactionManagerActionQueue().take();

                    // do we have any transaction-related event?
                    switch(action){
                        case BATCH_COMPLETE -> {

                            // what if ACKs from VMSs take too long? or never arrive?
                            // need to deal with intersecting batches? actually juts continue emitting for higher throughput

                            BatchComplete.BatchCompletePayload msg = txManagerCtx.batchCompleteEvents().remove();

                            BatchContext batchContext = batchContextMap.get( msg.batch() );

                            // only if it is not a duplicate vote
                            if( batchContext.missingVotes.remove( msg.vms() ) ){

                                // making this implement order-independent, so not assuming batch commit are received in order, although they are necessarily applied in order both here and in the VMSs
                                // is the current? this approach may miss a batch... so when the batchOffsetPendingCommit finishes, it must check the batch context match to see whether it is completed
                                if( batchContext.batchOffset == batchOffsetPendingCommit && batchContext.missingVotes.size() == 0 ){

                                    sendCommitRequestToVMSs(batchContext);

                                    batchOffsetPendingCommit++;

                                    // is the next batch completed already?
                                    batchContext = batchContextMap.get( batchOffsetPendingCommit );
                                    while(batchContext.missingVotes.size() == 0){

                                    }

                                }

                            }


                        }
                        case TX_ABORT -> {

                            // send abort to all VMSs...
                            // later we can optimize the number of messages since some VMSs may not need to receive this abort



                        }
                    }
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }

            }

        }

        // this could be asynchronously
        private void sendCommitRequestToVMSs(BatchContext batchContext){

        }

    }

    /**
     *  Should read in a proportion that matches the batch and heartbeat window, otherwise
     *  how long does it take to process a batch of input transactions?
     *  instead of trying to match the rate of processing, perhaps we can create read tasks
     */
    private void parseAndSendTransactionInputEvents(){

        if (transactionRequestsToParse.size() == 0) {
            return;
        }

        // the payload received in bytes is json format

        transactionRequestsToParse.drainTo(drainedTransactionRequests);

        List<TransactionInput> parsedTransactionRequests = new ArrayList<>( drainedTransactionRequests.size() );

        // do not send another until the last has been completed, so we can better adjust the rate of parsing
        for (byte[] drainedElement : drainedTransactionRequests) {
            String json = new String(drainedElement);

            // must also check whether the event is correct, that is, contains all events
            try {
                TransactionInput transactionInput = gson.fromJson(json, TransactionInput.class);
                // order by name, since we guarantee the topology input events are ordered by name
                transactionInput.events.sort( Comparator.comparing(o -> o.name) );
                parsedTransactionRequests.add(transactionInput);
            } catch(JsonSyntaxException ignored) {} // simply ignore
        }

        // clean for next iteration
        drainedTransactionRequests.clear();

        for(TransactionInput transactionInput : parsedTransactionRequests){

            // this is the only thread updating this value, so it is by design an atomic operation
            long tid_ = ++tid;

            TransactionDAG transactionDAG = transactionMap.get( transactionInput.name );

            // should find a way to continue emitting new transactions without stopping this thread
            // non-blocking design
            // having the batch here guarantees that all input events of the same tid does
            // not belong to different batches

            // to allow for a snapshot of the last TIDs of each vms involved in this transaction
            long batch_;
            synchronized (batchCommitLock) {
                // get the batch here since the batch handler is also incrementing it inside a synchronized block
                batch_ = batchOffset;
            }

            // for each input event, send the event to the proper vms
            // assuming the input is correct, i.e., all events are present
            for (TransactionInput.Event inputEvent : transactionInput.events) {

                // look for the event in the topology
                EventIdentifier event = transactionDAG.topology.get(inputEvent.name);

                // get the vms
                VmsIdentifier vms = VMSs.get(event.vms.hashCode());

                // get the connection metadata
                VmsConnectionMetadata connectionMetadata = vmsConnectionMetadataMap.get( vms.hashCode() );

                // we could employ deterministic writes to the channel, that is, an order that would not require locking for writes
                // we could possibly open a channel per write operation, but... what are the consequences of too much opened connections?
                connectionMetadata.writeLock.lock();

                // assign this event, so... what? try to send later?
                connectionMetadata.lastEventWritten = event;

                // write. think about failures/atomicity later
                TransactionEvent.write(connectionMetadata.writeBuffer, tid_, vms.lastTid, batch_, inputEvent.name, inputEvent.payload);

                // a vms, although receiving an event from a "next" batch, cannot yet commit, since
                // there may have additional events to arrive from the current batch
                // so the batch request must contain the last tid of the given vms

                // update for next transaction
                vms.lastTid = tid_;

                connectionMetadata.channel.write(connectionMetadata.writeBuffer, connectionMetadata, new WriteCompletionHandler());

            }

            // add terminal to the set... so cannot be immutable when the batch context is created...
            batchContextMap.get(batch_).terminalVMSs.addAll( transactionDAG.terminals );

        }

    }

}
